{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to bolt diy","text":"<p>bolt.diy allows you to choose the LLM that you use for each prompt! Currently, you can use OpenAI, Anthropic, Ollama, OpenRouter, Gemini, LMStudio, Mistral, xAI, HuggingFace, DeepSeek, or Groq models - and it is easily extended to use any other model supported by the Vercel AI SDK! See the instructions below for running this locally and extending it to include more models.</p>"},{"location":"#table-of-contents","title":"Table of Contents","text":"<ul> <li>Join the community!</li> <li>Features</li> <li>Setup</li> <li>Prerequisites</li> <li>Clone the Repository</li> <li>Entering API Keys<ul> <li>1. Set API Keys in the <code>.env.local</code> File</li> <li>2. Configure API Keys Directly in the Application</li> </ul> </li> <li>Run the Application</li> <li>Option 1: Without Docker</li> <li>Option 2: With Docker</li> <li>Update Your Local Version to the Latest</li> <li>Adding New LLMs</li> <li>Available Scripts</li> <li>Development</li> <li>Tips and Tricks</li> </ul>"},{"location":"#join-the-community","title":"Join the community!","text":"<p>Join the community!</p>"},{"location":"#features","title":"Features","text":"<ul> <li>AI-powered full-stack web development directly in your browser.</li> <li>Support for multiple LLMs with an extensible architecture to integrate additional models.</li> <li>Attach images to prompts for better contextual understanding.</li> <li>Integrated terminal to view output of LLM-run commands.</li> <li>Revert code to earlier versions for easier debugging and quicker changes.</li> <li>Download projects as ZIP for easy portability.</li> <li>Integration-ready Docker support for a hassle-free setup.</li> </ul>"},{"location":"#setup","title":"Setup","text":"<p>If you're new to installing software from GitHub, don't worry! If you encounter any issues, feel free to submit an \"issue\" using the provided links or improve this documentation by forking the repository, editing the instructions, and submitting a pull request. The following instruction will help you get the stable branch up and running on your local machine in no time.  </p>"},{"location":"#prerequisites","title":"Prerequisites","text":"<ol> <li>Install Git: Download Git </li> <li> <p>Install Node.js: Download Node.js </p> </li> <li> <p>After installation, the Node.js path is usually added to your system automatically. To verify:  </p> <ul> <li>Windows: Search for \"Edit the system environment variables,\" click \"Environment Variables,\" and check if <code>Node.js</code> is in the <code>Path</code> variable.  </li> <li>Mac/Linux: Open a terminal and run: <pre><code>echo $PATH  \n</code></pre>    Look for <code>/usr/local/bin</code> in the output.  </li> </ul> </li> </ol>"},{"location":"#clone-the-repository","title":"Clone the Repository","text":"<p>Alternatively, you can download the latest version of the project directly from the Releases Page. Simply download the .zip file, extract it, and proceed with the setup instructions below. If you are comfertiable using git then run the command below.</p> <p>Clone the repository using Git:  </p> <pre><code>git clone -b stable https://github.com/stackblitz-labs/bolt.diy  \n</code></pre>"},{"location":"#entering-api-keys","title":"Entering API Keys","text":"<p>There are two ways to configure your API keys in bolt.diy:</p>"},{"location":"#1-set-api-keys-in-the-envlocal-file","title":"1. Set API Keys in the <code>.env.local</code> File","text":"<p>When setting up the application, you will need to add your API keys for the LLMs you wish to use. You can do this by renaming the <code>.env.example</code> file to <code>.env.local</code> and adding your API keys there. </p> <ul> <li>On Mac, you can find the file at <code>[your name]/bolt.diy/.env.example</code>.</li> <li>On Windows/Linux, the path will be similar.</li> </ul> <p>If you can't see the file, it's likely because hidden files are not being shown. On Mac, open a Terminal window and enter the following command to show hidden files:</p> <pre><code>defaults write com.apple.finder AppleShowAllFiles YES\n</code></pre> <p>Make sure to add your API keys for each provider you want to use, for example:</p> <pre><code>GROQ_API_KEY=XXX\nOPENAI_API_KEY=XXX\nANTHROPIC_API_KEY=XXX\n</code></pre> <p>Once you've set your keys, you can proceed with running the app. You will set these keys up during the initial setup, and you can revisit and update them later after the app is running.</p> <p>Note: Never commit your <code>.env.local</code> file to version control. It\u2019s already included in the <code>.gitignore</code>.</p>"},{"location":"#2-configure-api-keys-directly-in-the-application","title":"2. Configure API Keys Directly in the Application","text":"<p>Alternatively, you can configure your API keys directly in the application once it's running. To do this:</p> <ol> <li>Launch the application and navigate to the provider selection dropdown.</li> <li>Select the provider you wish to configure.</li> <li>Click the pencil icon next to the selected provider.</li> <li>Enter your API key in the provided field.</li> </ol> <p>This method allows you to easily add or update your keys without needing to modify files directly.</p> <p>Once you've configured your keys, the application will be ready to use the selected LLMs.</p>"},{"location":"#run-the-application","title":"Run the Application","text":""},{"location":"#option-1-without-docker","title":"Option 1: Without Docker","text":"<ol> <li> <p>Install Dependencies: <pre><code>pnpm install  \n</code></pre>    If <code>pnpm</code> is not installed, install it using: <pre><code>sudo npm install -g pnpm  \n</code></pre></p> </li> <li> <p>Start the Application: <pre><code>pnpm run dev  \n</code></pre>    This will start the Remix Vite development server. You will need Google Chrome Canary to run this locally if you use Chrome! It's an easy install and a good browser for web development anyway.  </p> </li> </ol>"},{"location":"#option-2-with-docker","title":"Option 2: With Docker","text":""},{"location":"#prerequisites_1","title":"Prerequisites","text":"<ul> <li>Ensure Git, Node.js, and Docker are installed: Download Docker </li> </ul>"},{"location":"#steps","title":"Steps","text":"<ol> <li>Build the Docker Image:  </li> </ol> <p>Use the provided NPM scripts: <pre><code>npm run dockerbuild \n</code></pre></p> <p>Alternatively, use Docker commands directly: <pre><code>docker build . --target bolt-ai-development\n</code></pre></p> <ol> <li> <p>Run the Container:    Use Docker Compose profiles to manage environments: <pre><code>docker-compose --profile development up \n</code></pre></p> </li> <li> <p>With the development profile, changes to your code will automatically reflect in the running container (hot reloading).  </p> </li> </ol>"},{"location":"#update-your-local-version-to-the-latest","title":"Update Your Local Version to the Latest","text":"<p>To keep your local version of bolt.diy up to date with the latest changes, follow these steps for your operating system:</p>"},{"location":"#1-navigate-to-your-project-folder","title":"1. Navigate to your project folder","text":"<p>Navigate to the directory where you cloned the repository and open a terminal:</p>"},{"location":"#2-fetch-the-latest-changes","title":"2. Fetch the Latest Changes","text":"<p>Use Git to pull the latest changes from the main repository:</p> <pre><code>git pull origin main\n</code></pre>"},{"location":"#3-update-dependencies","title":"3. Update Dependencies","text":"<p>After pulling the latest changes, update the project dependencies by running the following command:</p> <pre><code>pnpm install\n</code></pre>"},{"location":"#4-rebuild-and-start-the-application","title":"4. Rebuild and Start the Application","text":"<ul> <li> <p>If using Docker, ensure you rebuild the Docker image to avoid using a cached version: <pre><code>docker-compose --profile development up --build  \n</code></pre></p> </li> <li> <p>If not using Docker, you can start the application as usual with: <pre><code>pnpm run dev  \n</code></pre></p> </li> </ul> <p>This ensures that you're running the latest version of bolt.diy and can take advantage of all the newest features and bug fixes. </p>"},{"location":"#adding-new-llms","title":"Adding New LLMs:","text":"<p>To make new LLMs available to use in this version of bolt.diy, head on over to <code>app/utils/constants.ts</code> and find the constant MODEL_LIST. Each element in this array is an object that has the model ID for the name (get this from the provider's API documentation), a label for the frontend model dropdown, and the provider. </p> <p>By default, Anthropic, OpenAI, Groq, and Ollama are implemented as providers, but the YouTube video for this repo covers how to extend this to work with more providers if you wish!</p> <p>When you add a new model to the MODEL_LIST array, it will immediately be available to use when you run the app locally or reload it. For Ollama models, make sure you have the model installed already before trying to use it here!</p>"},{"location":"#available-scripts","title":"Available Scripts","text":"<ul> <li><code>pnpm run dev</code>: Starts the development server.</li> <li><code>pnpm run build</code>: Builds the project.</li> <li><code>pnpm run start</code>: Runs the built application locally using Wrangler Pages. This script uses <code>bindings.sh</code> to set up necessary bindings so you don't have to duplicate environment variables.</li> <li><code>pnpm run preview</code>: Builds the project and then starts it locally, useful for testing the production build. Note, HTTP streaming currently doesn't work as expected with <code>wrangler pages dev</code>.</li> <li><code>pnpm test</code>: Runs the test suite using Vitest.</li> <li><code>pnpm run typecheck</code>: Runs TypeScript type checking.</li> <li><code>pnpm run typegen</code>: Generates TypeScript types using Wrangler.</li> <li><code>pnpm run deploy</code>: Builds the project and deploys it to Cloudflare Pages.</li> </ul>"},{"location":"#development","title":"Development","text":"<p>To start the development server:</p> <pre><code>pnpm run dev\n</code></pre> <p>This will start the Remix Vite development server. You will need Google Chrome Canary to run this locally if you use Chrome! It's an easy install and a good browser for web development anyway.</p>"},{"location":"#tips-and-tricks","title":"Tips and Tricks","text":"<p>Here are some tips to get the most out of bolt.diy:</p> <ul> <li> <p>Be specific about your stack: If you want to use specific frameworks or libraries (like Astro, Tailwind, ShadCN, or any other popular JavaScript framework), mention them in your initial prompt to ensure Bolt scaffolds the project accordingly.</p> </li> <li> <p>Use the enhance prompt icon: Before sending your prompt, try clicking the 'enhance' icon to have the AI model help you refine your prompt, then edit the results before submitting.</p> </li> <li> <p>Scaffold the basics first, then add features: Make sure the basic structure of your application is in place before diving into more advanced functionality. This helps Bolt understand the foundation of your project and ensure everything is wired up right before building out more advanced functionality.</p> </li> <li> <p>Batch simple instructions: Save time by combining simple instructions into one message. For example, you can ask Bolt to change the color scheme, add mobile responsiveness, and restart the dev server, all in one go saving you time and reducing API credit consumption significantly.</p> </li> </ul>"},{"location":"API/","title":"API","text":"<p>Documentation API de NeuroCode-ia NeuroCode-ia interagit avec plusieurs APIs, principalement pour la communication avec les LLMs et pour certaines fonctionnalit\u00e9s comme le clonage de projets Git. Voici une description d\u00e9taill\u00e9e des endpoints utilis\u00e9s :</p> <ol> <li>API de Chat (/api/chat) Cette API est le point d'entr\u00e9e principal pour l'interaction avec les LLMs.</li> </ol> <p>Endpoint: /api/chat</p> <p>M\u00e9thode: POST</p> <p>Requ\u00eate:</p> <p>{   \"messages\": [     {       \"role\": \"user\" | \"assistant\",       \"content\": \"string | array\", // Le contenu du message. Peut \u00eatre un tableau pour inclure des images.       \"id\": \"string\", // L'identifiant unique du message       \"annotations\": [] // Optionnel, informations suppl\u00e9mentaires sur le message.     }     // ... autres messages   ],   \"files\": {     // Mappage des chemins de fichiers vers leur contenu et type (facultatif)     \"/path/to/file.js\": {       \"type\": \"file\",       \"content\": \"string\", // Contenu du fichier       \"isBinary\": false     }   },   \"promptId\": \"string\", // Identifiant du prompt, si utilis\u00e9 (optionnel)   \"contextOptimization\": boolean, // Indique si l'optimisation de contexte est activ\u00e9e (optionnel)   \"model\": \"string\", // Mod\u00e8le de LLM \u00e0 utiliser (ex: \"gpt-4-turbo\")   \"provider\": { // Information sur le fournisseur de LLM     \"name\": \"string\", // Nom du fournisseur (ex: \"OpenAI\")     \"icon\": \"string\", // Ic\u00f4ne du fournisseur (optionnel)     \"getApiKeyLink\": \"string\", // Lien vers la page pour obtenir une cl\u00e9 API (optionnel)     \"labelForGetApiKey\": \"string\" // Texte \u00e0 afficher pour le lien vers la cl\u00e9 API (optionnel)   },   \"apiKeys\": {     // Cl\u00e9s API pour les diff\u00e9rents fournisseurs (optionnel)     \"OpenAI\": \"sk-...\",     \"Anthropic\": \"sk-ant-...\"     // ... autres cl\u00e9s API   },   \"providerSettings\": {     // Param\u00e8tres de configuration pour chaque fournisseur (optionnel)     \"OpenAI\": {       \"enabled\": true,       \"baseUrl\": \"https://api.openai.com/v1\"     },     \"Anthropic\": {       \"enabled\": true,       \"baseUrl\": \"https://api.anthropic.com/v1\"     }     // ... autres param\u00e8tres   } } Use code with caution. Json R\u00e9ponse: Un flux de texte (text/event-stream) contenant les r\u00e9ponses du LLM, potentiellement entrecoup\u00e9 de donn\u00e9es JSON encod\u00e9es en data: { ... }. La r\u00e9ponse peut inclure :</p> <p>Du texte g\u00e9n\u00e9r\u00e9 par le LLM.</p> <p>Des m\u00e9tadonn\u00e9es sur l'utilisation des tokens :</p> <p>data: {\"type\":\"usage\",\"value\":{\"completionTokens\":10,\"promptTokens\":20,\"totalTokens\":30}} Use code with caution. Json Des balises  et  pour les actions \u00e0 ex\u00e9cuter c\u00f4t\u00e9 client. <p>Exemple d'utilisation:</p> <p>const response = await fetch('/api/chat', {   method: 'POST',   headers: {     'Content-Type': 'application/json',   },   body: JSON.stringify({     messages: [       {         role: 'user',         content: 'Cr\u00e9er un fichier \"hello.js\" avec le contenu \"console.log(\\'Hello, world!\\');\"',         id: 'msg1',       },     ],     model: 'gpt-4',     provider: { name: 'OpenAI' },   }), });</p> <p>const reader = response.body.getReader();</p> <p>while (true) {   const { done, value } = await reader.read();   if (done) break;   // Traiter le flux de texte (value est un Uint8Array)   const text = new TextDecoder().decode(value);   console.log(text); } Use code with caution. JavaScript 2. API d'Am\u00e9lioration de Prompt (/api/enhancer) Cette API est utilis\u00e9e pour am\u00e9liorer les prompts fournis par l'utilisateur.</p> <p>Endpoint: /api/enhancer</p> <p>M\u00e9thode: POST</p> <p>Requ\u00eate:</p> <p>{   \"message\": \"string\", // Le prompt original de l'utilisateur   \"model\": \"string\", // Le mod\u00e8le de LLM \u00e0 utiliser pour l'am\u00e9lioration   \"provider\": { // Information sur le fournisseur de LLM     \"name\": \"string\" // Nom du fournisseur (ex: \"OpenAI\")   },   \"apiKeys\": {     // Cl\u00e9s API pour les diff\u00e9rents fournisseurs (optionnel)     \"OpenAI\": \"sk-...\",     \"Anthropic\": \"sk-ant-...\"     // ... autres cl\u00e9s API   } } Use code with caution. Json R\u00e9ponse: Un flux de texte (text/plain; charset=utf-8) contenant le prompt am\u00e9lior\u00e9.</p> <p>Exemple d'utilisation:</p> <p>const response = await fetch('/api/enhancer', {   method: 'POST',   headers: {     'Content-Type': 'application/json',   },   body: JSON.stringify({     message: 'Fais un jeu de snake',     model: 'claude-3-5-sonnet-latest',     provider: { name: 'Anthropic' },   }), });</p> <p>const reader = response.body.getReader(); let enhancedPrompt = '';</p> <p>while (true) {   const { done, value } = await reader.read();   if (done) break;   enhancedPrompt += new TextDecoder().decode(value); }</p> <p>console.log('Prompt am\u00e9lior\u00e9 :', enhancedPrompt); Use code with caution. JavaScript 3. API de Proxy Git (/api/git-proxy.$) Cette API sert de proxy pour les requ\u00eates Git vers des d\u00e9p\u00f4ts externes, contournant les restrictions CORS dans le navigateur.</p> <p>Endpoint: /api/git-proxy/* (o\u00f9 * est le reste de l'URL du d\u00e9p\u00f4t Git)</p> <p>M\u00e9thode: Toutes les m\u00e9thodes HTTP (GET, POST, etc.)</p> <p>Requ\u00eate: La requ\u00eate est transmise telle quelle au serveur Git, avec les en-t\u00eates CORS appropri\u00e9s.</p> <p>R\u00e9ponse: La r\u00e9ponse du serveur Git est transmise au client, avec les en-t\u00eates CORS appropri\u00e9s.</p> <p>Exemple d'utilisation (interne \u00e0 useGit):</p> <p>// La fonction <code>gitClone</code> dans <code>useGit</code> utilise cette API pour cloner un d\u00e9p\u00f4t const { workdir, data } = await gitClone('https://github.com/user/repo.git'); Use code with caution. JavaScript Note: Cette API n'est pas destin\u00e9e \u00e0 \u00eatre utilis\u00e9e directement par les utilisateurs, mais elle est cruciale pour le fonctionnement de la fonctionnalit\u00e9 de clonage de d\u00e9p\u00f4ts Git.</p> <ol> <li>API de liste des mod\u00e8les (/api/models) Cette API fournit la liste des mod\u00e8les disponibles, extraite de PROVIDER_LIST.</li> </ol> <p>Endpoint: /api/models</p> <p>M\u00e9thode: GET</p> <p>Requ\u00eate: Aucune</p> <p>R\u00e9ponse: Un objet JSON contenant la liste des mod\u00e8les disponibles.</p> <p>[   {     \"name\": \"claude-3-5-sonnet-latest\",     \"label\": \"Claude 3.5 Sonnet (new)\",     \"provider\": \"Anthropic\",     \"maxTokenAllowed\": 8000   },   {     \"name\": \"claude-3-5-sonnet-20240620\",     \"label\": \"Claude 3.5 Sonnet (old)\",     \"provider\": \"Anthropic\",     \"maxTokenAllowed\": 8000   },   // ... autres mod\u00e8les ] Use code with caution. Json Exemple d'utilisation:</p> <p>fetch('/api/models')   .then((response) =&gt; response.json())   .then((models) =&gt; {     console.log('Mod\u00e8les disponibles:', models);   }); Use code with caution. JavaScript 5. API d'appel LLM (/api/llmcall) Cette API permet d'appeler directement un LLM avec un prompt syst\u00e8me et un message utilisateur, et de recevoir une r\u00e9ponse en streaming ou en JSON.</p> <p>Endpoint: /api/llmcall</p> <p>M\u00e9thode: POST</p> <p>Requ\u00eate:</p> <p>{   \"system\": \"string\", // Prompt syst\u00e8me optionnel   \"message\": \"string\", // Message utilisateur   \"model\": \"string\", // Mod\u00e8le de LLM \u00e0 utiliser   \"provider\": { // Information sur le fournisseur de LLM     \"name\": \"string\" // Nom du fournisseur (ex: \"OpenAI\")   },   \"streamOutput\": boolean // Indique si la sortie doit \u00eatre stream\u00e9e (optionnel, d\u00e9faut: false) } Use code with caution. Json R\u00e9ponse:</p> <p>Si streamOutput est true: Un flux de texte (text/plain; charset=utf-8) contenant la r\u00e9ponse du LLM.</p> <p>Si streamOutput est false: Un objet JSON avec la r\u00e9ponse du LLM.</p> <p>{   \"text\": \"R\u00e9ponse du LLM\" } Use code with caution. Json Exemple d'utilisation (streaming):</p> <p>const response = await fetch('/api/llmcall', {   method: 'POST',   headers: {     'Content-Type': 'application/json',   },   body: JSON.stringify({     system: 'Tu es un assistant serviable.',     message: '\u00c9cris un po\u00e8me sur la nature.',     model: 'claude-3-5-sonnet-latest',     provider: { name: 'Anthropic' },     streamOutput: true,   }), });</p> <p>const reader = response.body.getReader();</p> <p>while (true) {   const { done, value } = await reader.read();   if (done) break;   const text = new TextDecoder().decode(value);   console.log('Texte re\u00e7u:', text); } Use code with caution. JavaScript Exemple d'utilisation (JSON):</p> <p>const response = await fetch('/api/llmcall', {   method: 'POST',   headers: {     'Content-Type': 'application/json',   },   body: JSON.stringify({     system: 'Tu es un assistant serviable.',     message: '\u00c9cris un po\u00e8me sur la nature.',     model: 'gpt-4',     provider: { name: 'OpenAI' },     streamOutput: false,   }), });</p> <p>const result = await response.json(); console.log('R\u00e9ponse du LLM:', result.text); Use code with caution. JavaScript Notes:</p> <p>Les cl\u00e9s API et les param\u00e8tres des fournisseurs sont g\u00e9r\u00e9s par les cookies, comme dans l'API /api/chat.</p> <p>Cette API est utilis\u00e9e en interne par la fonction selectStarterTemplate pour choisir un template de d\u00e9marrage.</p> <ol> <li>API non document\u00e9es Il existe d'autres APIs non document\u00e9es dans le code source, comme celles li\u00e9es \u00e0 la gestion des fichiers et \u00e0 la synchronisation avec le syst\u00e8me de fichiers local. Ces APIs ne sont pas destin\u00e9es \u00e0 \u00eatre utilis\u00e9es directement par les utilisateurs finaux.</li> </ol>"},{"location":"CONTRIBUTING/","title":"Contribution Guidelines","text":"<p>Welcome! This guide provides all the details you need to contribute effectively to the project. Thank you for helping us make bolt.diy a better tool for developers worldwide. \ud83d\udca1</p>"},{"location":"CONTRIBUTING/#table-of-contents","title":"\ud83d\udccb Table of Contents","text":"<ol> <li>Code of Conduct </li> <li>How Can I Contribute? </li> <li>Pull Request Guidelines </li> <li>Coding Standards </li> <li>Development Setup </li> <li>Testing </li> <li>Deployment </li> <li>Docker Deployment </li> <li>VS Code Dev Containers Integration </li> </ol>"},{"location":"CONTRIBUTING/#code-of-conduct","title":"\ud83d\udee1\ufe0f Code of Conduct","text":"<p>This project is governed by our Code of Conduct. By participating, you agree to uphold this code. Report unacceptable behavior to the project maintainers.</p>"},{"location":"CONTRIBUTING/#how-can-i-contribute","title":"\ud83d\udee0\ufe0f How Can I Contribute?","text":""},{"location":"CONTRIBUTING/#1-reporting-bugs-or-feature-requests","title":"1\ufe0f\u20e3 Reporting Bugs or Feature Requests","text":"<ul> <li>Check the issue tracker to avoid duplicates.</li> <li>Use issue templates (if available).  </li> <li>Provide detailed, relevant information and steps to reproduce bugs.</li> </ul>"},{"location":"CONTRIBUTING/#2-code-contributions","title":"2\ufe0f\u20e3 Code Contributions","text":"<ol> <li>Fork the repository.  </li> <li>Create a feature or fix branch.  </li> <li>Write and test your code.  </li> <li>Submit a pull request (PR).</li> </ol>"},{"location":"CONTRIBUTING/#3-join-as-a-core-contributor","title":"3\ufe0f\u20e3 Join as a Core Contributor","text":"<p>Interested in maintaining and growing the project? Fill out our Contributor Application Form.</p>"},{"location":"CONTRIBUTING/#pull-request-guidelines","title":"\u2705 Pull Request Guidelines","text":""},{"location":"CONTRIBUTING/#pr-checklist","title":"PR Checklist","text":"<ul> <li>Branch from the main branch.  </li> <li>Update documentation, if needed.  </li> <li>Test all functionality manually.  </li> <li>Focus on one feature/bug per PR.  </li> </ul>"},{"location":"CONTRIBUTING/#review-process","title":"Review Process","text":"<ol> <li>Manual testing by reviewers.  </li> <li>At least one maintainer review required.  </li> <li>Address review comments.  </li> <li>Maintain a clean commit history.</li> </ol>"},{"location":"CONTRIBUTING/#coding-standards","title":"\ud83d\udccf Coding Standards","text":""},{"location":"CONTRIBUTING/#general-guidelines","title":"General Guidelines","text":"<ul> <li>Follow existing code style.  </li> <li>Comment complex logic.  </li> <li>Keep functions small and focused.  </li> <li>Use meaningful variable names.</li> </ul>"},{"location":"CONTRIBUTING/#development-setup","title":"\ud83d\udda5\ufe0f Development Setup","text":""},{"location":"CONTRIBUTING/#1-initial-setup","title":"1\ufe0f\u20e3 Initial Setup","text":"<ul> <li>Clone the repository: <pre><code>git clone https://github.com/stackblitz-labs/bolt.diy.git\n</code></pre></li> <li>Install dependencies: <pre><code>pnpm install\n</code></pre></li> <li>Set up environment variables:  </li> <li>Rename <code>.env.example</code> to <code>.env.local</code>.  </li> <li>Add your API keys:      <pre><code>GROQ_API_KEY=XXX\nHuggingFace_API_KEY=XXX\nOPENAI_API_KEY=XXX\n...\n</code></pre></li> <li>Optionally set:  <ul> <li>Debug level: <code>VITE_LOG_LEVEL=debug</code> </li> <li>Context size: <code>DEFAULT_NUM_CTX=32768</code> </li> </ul> </li> </ul> <p>Note: Never commit your <code>.env.local</code> file to version control. It\u2019s already in <code>.gitignore</code>.</p>"},{"location":"CONTRIBUTING/#2-run-development-server","title":"2\ufe0f\u20e3 Run Development Server","text":"<p><pre><code>pnpm run dev\n</code></pre> Tip: Use Google Chrome Canary for local testing.</p>"},{"location":"CONTRIBUTING/#testing","title":"\ud83e\uddea Testing","text":"<p>Run the test suite with: <pre><code>pnpm test\n</code></pre></p>"},{"location":"CONTRIBUTING/#deployment","title":"\ud83d\ude80 Deployment","text":""},{"location":"CONTRIBUTING/#deploy-to-cloudflare-pages","title":"Deploy to Cloudflare Pages","text":"<p><pre><code>pnpm run deploy\n</code></pre> Ensure you have required permissions and that Wrangler is configured.</p>"},{"location":"CONTRIBUTING/#docker-deployment","title":"\ud83d\udc33 Docker Deployment","text":"<p>This section outlines the methods for deploying the application using Docker. The processes for Development and Production are provided separately for clarity.</p>"},{"location":"CONTRIBUTING/#development-environment","title":"\ud83e\uddd1\u200d\ud83d\udcbb Development Environment","text":""},{"location":"CONTRIBUTING/#build-options","title":"Build Options","text":"<p>Option 1: Helper Scripts <pre><code># Development build\nnpm run dockerbuild\n</code></pre></p> <p>Option 2: Direct Docker Build Command <pre><code>docker build . --target bolt-ai-development\n</code></pre></p> <p>Option 3: Docker Compose Profile <pre><code>docker-compose --profile development up\n</code></pre></p>"},{"location":"CONTRIBUTING/#running-the-development-container","title":"Running the Development Container","text":"<pre><code>docker run -p 5173:5173 --env-file .env.local bolt-ai:development\n</code></pre>"},{"location":"CONTRIBUTING/#production-environment","title":"\ud83c\udfed Production Environment","text":""},{"location":"CONTRIBUTING/#build-options_1","title":"Build Options","text":"<p>Option 1: Helper Scripts <pre><code># Production build\nnpm run dockerbuild:prod\n</code></pre></p> <p>Option 2: Direct Docker Build Command <pre><code>docker build . --target bolt-ai-production\n</code></pre></p> <p>Option 3: Docker Compose Profile <pre><code>docker-compose --profile production up\n</code></pre></p>"},{"location":"CONTRIBUTING/#running-the-production-container","title":"Running the Production Container","text":"<pre><code>docker run -p 5173:5173 --env-file .env.local bolt-ai:production\n</code></pre>"},{"location":"CONTRIBUTING/#coolify-deployment","title":"Coolify Deployment","text":"<p>For an easy deployment process, use Coolify: 1. Import your Git repository into Coolify. 2. Choose Docker Compose as the build pack. 3. Configure environment variables (e.g., API keys). 4. Set the start command: <pre><code>docker compose --profile production up\n</code></pre></p>"},{"location":"CONTRIBUTING/#vs-code-dev-containers-integration","title":"\ud83d\udee0\ufe0f VS Code Dev Containers Integration","text":"<p>The <code>docker-compose.yaml</code> configuration is compatible with VS Code Dev Containers, making it easy to set up a development environment directly in Visual Studio Code.</p>"},{"location":"CONTRIBUTING/#steps-to-use-dev-containers","title":"Steps to Use Dev Containers","text":"<ol> <li>Open the command palette in VS Code (<code>Ctrl+Shift+P</code> or <code>Cmd+Shift+P</code> on macOS).  </li> <li>Select Dev Containers: Reopen in Container.  </li> <li>Choose the development profile when prompted.  </li> <li>VS Code will rebuild the container and open it with the pre-configured environment.</li> </ol>"},{"location":"CONTRIBUTING/#environment-variables","title":"\ud83d\udd11 Environment Variables","text":"<p>Ensure <code>.env.local</code> is configured correctly with: - API keys. - Context-specific configurations.  </p> <p>Example for the <code>DEFAULT_NUM_CTX</code> variable: <pre><code>DEFAULT_NUM_CTX=24576 # Uses 32GB VRAM\n</code></pre></p>"},{"location":"DIAGRAMME/","title":"DIAGRAMME","text":"<p>Diagrammes UML pour NeuroCode-ia Voici quelques diagrammes UML pour illustrer la structure et les interactions cl\u00e9s de NeuroCode-ia.</p> <ol> <li>Diagramme de Classes (Simplifi\u00e9) Ce diagramme montre les classes principales et leurs relations.</li> </ol> <p>@startuml class Chat {   -initialMessages: Message[]   -storeMessageHistory(messages: Message[]): Promise   -importChat(description: string, messages: Message[]): Promise   -exportChat(): void } <p>class Workbench {   -files: FileMap   -unsavedFiles: Set   -selectedFile: string   -currentDocument: EditorDocument   -showWorkbench: boolean   -currentView: WorkbenchViewType   -previews: PreviewInfo[]   -actionAlert: ActionAlert   -boltTerminal: ITerminal   +addAction(data: ActionCallbackData)   +runAction(data: ActionCallbackData, isStreaming: boolean)   +setSelectedFile(filePath: string)   +saveFile(filePath: string, content: string)   +saveAllFiles()   +resetCurrentDocument()   +downloadZip()   +pushToGitHub(repoName: string, username: string, token: string)   +toggleTerminal(value?: boolean) } <p>class EditorPanel {   -files: FileMap   -unsavedFiles: Set   -editorDocument: EditorDocument   -selectedFile: string   -isStreaming: boolean   +onEditorChange(update: EditorUpdate)   +onEditorScroll(position: ScrollPosition)   +onFileSelect(value?: string)   +onFileSave()   +onFileReset() } <p>class FileTree {   -files: FileMap   -selectedFile: string   -rootFolder: string   -hideRoot: boolean   -collapsed: boolean   +onFileSelect(filePath: string) }</p> <p>class Terminal {   -terminal: ITerminal   +write(data: string)   +onData(callback: (data: string) =&gt; void) }</p> <p>class API {   +POST /api/chat   +POST /api/enhancer   +GET /api/models   +ANY /api/git-proxy/* }</p> <p>class ActionRunner {   -webcontainer: Promise   -currentExecutionPromise: Promise   -shellTerminal: () =&gt; BoltShell   +actions: ActionsMap   +runnerId: WritableAtom   +onAlert: (alert: ActionAlert) =&gt; void   +addAction(data: ActionCallbackData)   +runAction(data: ActionCallbackData, isStreaming: boolean)   -executeAction(actionId: string, isStreaming: boolean)   -runShellAction(action: ActionState)   -runStartAction(action: ActionState)   -runFileAction(action: ActionState)   -updateAction(id: string, newState: ActionStateUpdate)   +abortAllActions()   +setReloadedMessages(messages: string[]) } <p>Chat -- Workbench : uses Workbench o-- EditorPanel : has Workbench \"1\" o-- \"0..\" Terminal : has EditorPanel o-- FileTree : has API -- Chat : interacts with @enduml Use code with caution. Plantuml Explication du diagramme de classes :</p> <p>Chat: Composant principal de l'interface de chat.</p> <p>Workbench: G\u00e8re l'\u00e9tat global de l'atelier (fichiers, \u00e9diteur, terminal, aper\u00e7u).</p> <p>EditorPanel: G\u00e8re l'affichage de l'\u00e9diteur de code et de l'arborescence des fichiers.</p> <p>FileTree: Affiche l'arborescence des fichiers.</p> <p>Terminal: G\u00e8re un terminal unique.</p> <p>API: Repr\u00e9sente les endpoints de l'API REST.</p> <p>ActionRunner: G\u00e8re l'ex\u00e9cution des actions sugg\u00e9r\u00e9es par le LLM.</p> <ol> <li>Diagramme de S\u00e9quence (Cr\u00e9ation d'un fichier) Ce diagramme illustre la s\u00e9quence d'actions lors de la cr\u00e9ation d'un fichier par le LLM.</li> </ol> <p>@startuml participant User as U participant Chat as C participant API as A participant LLM participant ActionRunner as AR participant Workbench as W participant FilesStore as FS participant WebContainer as WC</p> <p>U -&gt; C: Envoie un prompt \"Cr\u00e9e un fichier index.html\" activate C C -&gt; A: POST /api/chat {messages: [...]} activate A A -&gt; LLM: Envoie le prompt au LLM activate LLM LLM --&gt; A: R\u00e9ponse en streaming avec <code>&lt;boltArtifact&gt;</code> et <code>&lt;boltAction type=\"file\"&gt;</code> deactivate LLM A -&gt; AR: onArtifactOpen(...) activate AR AR -&gt; W: addArtifact(...) activate W W -&gt; W: set showWorkbench = true deactivate W A -&gt; AR: onActionOpen({type: \"file\", filePath: \"index.html\", content: \"...\"}) AR -&gt; AR: addAction(...) A -&gt; C: Envoie le flux de texte deactivate A C -&gt; U: Affiche le texte loop pour chaque chunk de texte   A -&gt; C: Envoie le chunk de texte   C -&gt; C: parseMessages(...)   C -&gt; U: Met \u00e0 jour l'affichage end A -&gt; AR: onActionClose(...) AR -&gt; AR: runAction(...) activate AR AR -&gt; FS: saveFile(\"index.html\", \"...\") activate FS FS -&gt; WC: writeFile(\"index.html\", \"...\") activate WC WC --&gt; FS: Succ\u00e8s deactivate WC FS -&gt; W: met \u00e0 jour files deactivate FS W -&gt; W: met \u00e0 jour l'\u00e9tat (unsavedFiles) deactivate W AR --&gt; A: Succ\u00e8s deactivate AR A -&gt; C: Envoie \"usage\" data C --&gt; U: Affiche la r\u00e9ponse compl\u00e8te deactivate C</p> <p>@enduml Use code with caution. Plantuml Explication du diagramme de s\u00e9quence :</p> <p>L'utilisateur envoie un prompt pour cr\u00e9er un fichier.</p> <p>Le composant Chat envoie la requ\u00eate \u00e0 l'API /api/chat.</p> <p>L'API communique avec le LLM.</p> <p>Le LLM renvoie une r\u00e9ponse en streaming.</p> <p>StreamingMessageParser parse la r\u00e9ponse et appelle les callbacks onArtifactOpen et onActionOpen pour ActionRunner.</p> <p>ActionRunner ajoute l'artefact et l'action \u00e0 son \u00e9tat interne.</p> <p>ActionRunner ex\u00e9cute l'action de type \"file\".</p> <p>FilesStore \u00e9crit le contenu du fichier dans le WebContainer.</p> <p>Workbench met \u00e0 jour l'\u00e9tat files et unsavedFiles.</p> <p>L'API envoie les informations d'utilisation des jetons au frontend.</p> <ol> <li>Diagramme d'Activit\u00e9 (Am\u00e9lioration de Prompt) Ce diagramme illustre le processus d'am\u00e9lioration d'un prompt utilisateur.</li> </ol> <p>@startuml start :Utilisateur saisit un prompt; :Clic sur \"Am\u00e9liorer le prompt\"; :Composant Chat appelle usePromptEnhancer.enhancePrompt(); :Affichage de l'indicateur de chargement; :Requ\u00eate POST vers /api/enhancer; fork     :API appelle le LLM avec le prompt syst\u00e8me d'am\u00e9lioration; fork again     :R\u00e9ception du flux de texte am\u00e9lior\u00e9;     :Mise \u00e0 jour progressive du prompt dans la zone de texte; end fork :Fin du streaming; :Masquage de l'indicateur de chargement; :Utilisateur peut modifier le prompt am\u00e9lior\u00e9; :Utilisateur envoie le prompt; stop @enduml Use code with caution. Plantuml Explication du diagramme d'activit\u00e9 :</p> <p>L'utilisateur saisit un prompt dans la zone de texte.</p> <p>L'utilisateur clique sur le bouton \"Am\u00e9liorer le prompt\".</p> <p>La fonction enhancePrompt du hook usePromptEnhancer est appel\u00e9e.</p> <p>Un indicateur de chargement est affich\u00e9.</p> <p>Une requ\u00eate POST est envoy\u00e9e \u00e0 l'API /api/enhancer.</p> <p>L'API appelle le LLM avec le prompt syst\u00e8me d'am\u00e9lioration et le prompt utilisateur.</p> <p>Le LLM renvoie le prompt am\u00e9lior\u00e9 en streaming.</p> <p>La zone de texte est mise \u00e0 jour progressivement avec le prompt am\u00e9lior\u00e9.</p> <p>L'indicateur de chargement est masqu\u00e9.</p> <p>L'utilisateur peut modifier le prompt am\u00e9lior\u00e9 avant de l'envoyer.</p> <p>Ces diagrammes ne repr\u00e9sentent qu'une partie de l'architecture et du design de NeuroCode-ia. Ils visent \u00e0 donner une compr\u00e9hension g\u00e9n\u00e9rale des interactions cl\u00e9s. Des diagrammes plus d\u00e9taill\u00e9s pourraient \u00eatre cr\u00e9\u00e9s pour des composants sp\u00e9cifiques ou des cas d'utilisation particuliers.</p>"},{"location":"FAQ/","title":"Questions Fr\u00e9quemment Pos\u00e9es (FAQ)","text":""},{"location":"FAQ/#quels-sont-les-meilleurs-modeles-pour-boltdiy","title":"Quels sont les meilleurs mod\u00e8les pour bolt.diy ?","text":"<p>Pour la meilleure exp\u00e9rience avec bolt.diy, nous recommandons d'utiliser les mod\u00e8les suivants :</p> <ul> <li>Claude 3.5 Sonnet (ancien) : Meilleur codeur global, fournissant d'excellents r\u00e9sultats dans tous les cas d'utilisation</li> <li>Gemini 2.0 Flash : Vitesse exceptionnelle tout en maintenant de bonnes performances</li> <li>GPT-4o : Solide alternative \u00e0 Claude 3.5 Sonnet avec des capacit\u00e9s comparables</li> <li>DeepSeekCoder V2 236b : Meilleur mod\u00e8le open source (disponible via OpenRouter, DeepSeek API, ou auto-h\u00e9berg\u00e9)</li> <li>Qwen 2.5 Coder 32b : Meilleur mod\u00e8le pour l'auto-h\u00e9bergement avec des exigences mat\u00e9rielles raisonnables</li> </ul> <p>Note : Les mod\u00e8les avec moins de 7b param\u00e8tres manquent g\u00e9n\u00e9ralement de la capacit\u00e9 d'interagir correctement avec bolt !</p>"},{"location":"FAQ/#comment-obtenir-les-meilleurs-resultats-avec-boltdiy","title":"Comment obtenir les meilleurs r\u00e9sultats avec bolt.diy ?","text":"<ul> <li> <p>Soyez pr\u00e9cis sur votre stack :   Mentionnez les frameworks ou biblioth\u00e8ques que vous souhaitez utiliser (ex : Astro, Tailwind, ShadCN) dans votre prompt initial. Cela garantit que bolt.diy structure le projet selon vos pr\u00e9f\u00e9rences.</p> </li> <li> <p>Utilisez l'ic\u00f4ne d'am\u00e9lioration du prompt :   Avant d'envoyer votre prompt, cliquez sur l'ic\u00f4ne am\u00e9liorer pour laisser l'IA affiner votre prompt. Vous pouvez modifier les am\u00e9liorations sugg\u00e9r\u00e9es avant de les soumettre.</p> </li> <li> <p>Construisez d'abord les bases, puis ajoutez les fonctionnalit\u00e9s :   Assurez-vous que la structure fondamentale de votre application est en place avant d'introduire des fonctionnalit\u00e9s avanc\u00e9es. Cela aide bolt.diy \u00e0 \u00e9tablir une base solide sur laquelle construire.</p> </li> <li> <p>Regroupez les instructions simples :   Combinez les t\u00e2ches simples en un seul prompt pour gagner du temps et r\u00e9duire la consommation de cr\u00e9dits API. Par exemple : \"Changez le sch\u00e9ma de couleurs, ajoutez la r\u00e9activit\u00e9 mobile et red\u00e9marrez le serveur de d\u00e9veloppement.\"</p> </li> </ul>"},{"location":"FAQ/#comment-contribuer-a-boltdiy","title":"Comment contribuer \u00e0 bolt.diy ?","text":"<p>Consultez notre Guide de Contribution pour plus de d\u00e9tails sur la fa\u00e7on de participer !</p>"},{"location":"FAQ/#quels-sont-les-plans-futurs-pour-boltdiy","title":"Quels sont les plans futurs pour bolt.diy ?","text":"<p>Visitez notre Feuille de route pour les derni\u00e8res mises \u00e0 jour. De nouvelles fonctionnalit\u00e9s et am\u00e9liorations sont en cours !</p>"},{"location":"FAQ/#pourquoi-y-a-t-il-tant-de-problemespull-requests-ouverts","title":"Pourquoi y a-t-il tant de probl\u00e8mes/pull requests ouverts ?","text":"<p>bolt.diy a commenc\u00e9 comme un petit projet de d\u00e9monstration sur la cha\u00eene YouTube de @ColeMedin pour explorer l'\u00e9dition de projets open-source avec des LLMs locaux. Cependant, il s'est rapidement transform\u00e9 en un effort communautaire massif !</p> <p>Nous formons une \u00e9quipe de mainteneurs pour g\u00e9rer la demande et rationaliser la r\u00e9solution des probl\u00e8mes. Les mainteneurs sont des stars, et nous explorons \u00e9galement des partenariats pour aider le projet \u00e0 prosp\u00e9rer.</p>"},{"location":"FAQ/#comment-les-llms-locaux-se-comparent-ils-aux-modeles-plus-grands-comme-claude-35-sonnet-pour-boltdiy","title":"Comment les LLMs locaux se comparent-ils aux mod\u00e8les plus grands comme Claude 3.5 Sonnet pour bolt.diy ?","text":"<p>Bien que les LLMs locaux s'am\u00e9liorent rapidement, les mod\u00e8les plus grands comme GPT-4o, Claude 3.5 Sonnet et DeepSeek Coder V2 236b offrent toujours les meilleurs r\u00e9sultats pour les applications complexes. Notre objectif continu est d'am\u00e9liorer les prompts, les agents et la plateforme pour mieux supporter les LLMs locaux plus petits.</p>"},{"location":"FAQ/#erreurs-courantes-et-depannage","title":"Erreurs Courantes et D\u00e9pannage","text":""},{"location":"FAQ/#une-erreur-sest-produite-lors-du-traitement-de-cette-requete","title":"\"Une erreur s'est produite lors du traitement de cette requ\u00eate\"","text":"<p>Ce message d'erreur g\u00e9n\u00e9rique signifie que quelque chose s'est mal pass\u00e9. V\u00e9rifiez : - Le terminal (si vous avez d\u00e9marr\u00e9 l'application avec Docker ou <code>pnpm</code>). - La console d\u00e9veloppeur de votre navigateur (appuyez sur <code>F12</code> ou clic droit &gt; Inspecter, puis allez dans l'onglet Console).</p>"},{"location":"FAQ/#en-tete-x-api-key-manquant","title":"\"En-t\u00eate x-api-key manquant\"","text":"<p>Cette erreur est parfois r\u00e9solue en red\u00e9marrant le conteneur Docker. Si cela ne fonctionne pas, essayez de passer de Docker \u00e0 <code>pnpm</code> ou vice versa. Nous enqu\u00eatons activement sur ce probl\u00e8me.</p>"},{"location":"FAQ/#apercu-vide-lors-de-lexecution-de-lapplication","title":"Aper\u00e7u vide lors de l'ex\u00e9cution de l'application","text":"<p>Un aper\u00e7u vide survient souvent en raison de code hallucin\u00e9 ou de commandes incorrectes. Pour d\u00e9panner : - V\u00e9rifiez la console d\u00e9veloppeur pour les erreurs. - Rappelez-vous, les aper\u00e7us sont une fonctionnalit\u00e9 centrale, donc l'application n'est pas cass\u00e9e ! Nous travaillons \u00e0 rendre ces erreurs plus transparentes.</p>"},{"location":"FAQ/#tout-fonctionne-mais-les-resultats-sont-mauvais","title":"\"Tout fonctionne, mais les r\u00e9sultats sont mauvais\"","text":"<p>Les LLMs locaux comme Qwen-2.5-Coder sont puissants pour les petites applications mais encore exp\u00e9rimentaux pour les projets plus importants. Pour de meilleurs r\u00e9sultats, envisagez d'utiliser des mod\u00e8les plus grands comme GPT-4o, Claude 3.5 Sonnet, ou DeepSeek Coder V2 236b.</p>"},{"location":"FAQ/#exception-structuree-recue-0xc0000005-violation-dacces","title":"\"Exception structur\u00e9e re\u00e7ue #0xc0000005 : violation d'acc\u00e8s\"","text":"<p>Si vous obtenez ceci, vous \u00eates probablement sous Windows. La solution consiste g\u00e9n\u00e9ralement \u00e0 mettre \u00e0 jour le Visual C++ Redistributable</p>"},{"location":"FAQ/#erreurs-miniflare-ou-wrangler-sous-windows","title":"\"Erreurs Miniflare ou Wrangler sous Windows\"","text":"<p>Vous devrez vous assurer d'avoir la derni\u00e8re version de Visual Studio C++ install\u00e9e (14.40.33816), plus d'informations ici https://github.com/stackblitz-labs/bolt.diy/issues/19.</p> <p>Vous avez d'autres questions ? N'h\u00e9sitez pas \u00e0 nous contacter ou \u00e0 ouvrir une issue sur notre repo GitHub !</p>"},{"location":"SPECS/","title":"SPECS","text":"<p>Documentation Technique et de Conception Sp\u00e9cifications Techniques (Specs) Frameworks utilis\u00e9s : Remix (React) : Framework full-stack assurant \u00e0 la fois le rendu c\u00f4t\u00e9 serveur et l\u2019hydratation c\u00f4t\u00e9 client. Vite : Outil de build rapide et l\u00e9ger pour une exp\u00e9rience de d\u00e9veloppement fluide. UnoCSS : Moteur CSS atomique pour un style performant et modulable. CodeMirror 6 : \u00c9diteur de code int\u00e9gr\u00e9 offrant coloration syntaxique, autocompl\u00e9tion et autres fonctionnalit\u00e9s avanc\u00e9es. WebContainers : Conteneur d\u2019ex\u00e9cution Node.js c\u00f4t\u00e9 client, permettant d\u2019ex\u00e9cuter du code sans serveur distant. Radix UI : Biblioth\u00e8que de composants UI accessibles et modulaires pour React. Framer Motion : Biblioth\u00e8que pour des animations fluides et performantes. React Toastify : Gestion des notifications utilisateur de mani\u00e8re intuitive. Configuration du syst\u00e8me : Node.js : Environnement d\u2019ex\u00e9cution JavaScript c\u00f4t\u00e9 serveur (version LTS recommand\u00e9e). pnpm : Gestionnaire de packages rapide et efficace (alternative \u00e0 npm/yarn). Cloudflare Workers : Plateforme serverless utilis\u00e9e pour le d\u00e9ploiement en production. IndexedDB : Base de donn\u00e9es c\u00f4t\u00e9 client, utilis\u00e9e pour persister l\u2019historique des conversations et divers param\u00e8tres. APIs int\u00e9gr\u00e9es : Fournisseurs de LLMs : OpenAI, Anthropic, Cohere, Google, Groq, HuggingFace, Hyper, LM Studio, Mistral, Ollama, OpenRouter, OpenAI, Perplexity, xAI, Together, etc. API GitHub : Pour le clonage de d\u00e9p\u00f4ts, la cr\u00e9ation et la gestion de repos directement depuis l\u2019application. API Web Speech : Permet l\u2019int\u00e9gration de la reconnaissance vocale et de la synth\u00e8se vocale dans l\u2019interface (selon compatibilit\u00e9 navigateur). Architecture et Conception Globale : Point d\u2019entr\u00e9e API :</p> <p>neu/api/chat communique avec les diff\u00e9rents fournisseurs de LLMs. Les donn\u00e9es sont persist\u00e9es dans IndexedDB c\u00f4t\u00e9 client (historique, pr\u00e9f\u00e9rences, etc.). Sch\u00e9ma de la Base de Donn\u00e9es IndexedDB (boltHistory)</p> <p>Table : chats Colonne Type    Description id  TEXT    Identifiant unique de la conversation (auto-g\u00e9n\u00e9r\u00e9) urlId   TEXT    Identifiant unique pour l\u2019URL de la conversation (auto-g\u00e9n\u00e9r\u00e9) description TEXT    Br\u00e8ve description de la conversation messages    JSON    Tableau d\u2019objets Messages (format Vercel AI SDK) timestamp   TEXT    Date et heure de la derni\u00e8re mise \u00e0 jour (ISO 8601) Interactions entre les composants :</p> <p>Chat.client.tsx : Composant principal de l\u2019interface de chat (envoi/r\u00e9ception de messages, s\u00e9lection du mod\u00e8le, import/export de discussions, etc.). BaseChat.tsx : Structure de base pour l\u2019interface de chat. G\u00e8re zone de saisie, envoi de messages et gestion de fichiers. Messages.client.tsx : Affiche la liste des messages (utilisateur/assistant) avec fonctionnalit\u00e9s de retour et duplication. AssistantMessage.tsx / UserMessage.tsx : G\u00e8rent respectivement l\u2019affichage des r\u00e9ponses de l\u2019IA (voix, contenu, jetons) et des messages utilisateur (texte, images). APIKeyManager.tsx : Interface de gestion et de validation des cl\u00e9s API pour chaque fournisseur. Artifact.tsx : Affiche les \u201cartefacts\u201d g\u00e9n\u00e9r\u00e9s par l\u2019IA (fichiers, commandes, actions). Workbench.client.tsx : G\u00e8re l\u2019affichage de l\u2019\u00e9diteur de code, de l\u2019arborescence des fichiers, du terminal et de l\u2019aper\u00e7u (preview). EditorPanel.tsx, FileTree.tsx, EditorTabs.tsx : G\u00e8rent respectivement la zone d\u2019\u00e9dition, l\u2019affichage des fichiers et les onglets de fichiers ouverts. Terminal.tsx / Preview.tsx : G\u00e8rent l\u2019interface console et le rendu de l\u2019application en temps r\u00e9el. Menu.client.tsx, SettingsWindow.tsx, HelpWindow.tsx : G\u00e8rent les fonctionnalit\u00e9s du menu lat\u00e9ral, les param\u00e8tres, et l\u2019aide. GitUrlImport.client.tsx : G\u00e8re l\u2019importation d\u2019un projet depuis une URL Git. Choix architecturaux :</p> <p>Remix : Pour une exp\u00e9rience full-stack (SSR + hydratation c\u00f4t\u00e9 client). Vite : Pour un d\u00e9marrage et un build rapides. WebContainers : Alternative l\u00e9g\u00e8re aux environnements traditionnels (ex\u00e9cutions Node dans le navigateur). IndexedDB : Pour stocker localement l\u2019historique et r\u00e9duire la d\u00e9pendance serveur. UnoCSS : Style atomique pour de meilleures performances CSS. Radix UI + Framer Motion : Composants modulaires, animations fluides, et accessibilit\u00e9 renforc\u00e9e. Document de Conception (Design Document) Algorithmes et Logique : Gestion des Fichiers :</p> <p>Utilisation d\u2019une structure FileMap (JS) pour stocker fichiers et dossiers. Les modifications sont traqu\u00e9es dans modifiedFiles en comparant contenu actuel et initial. Parcours r\u00e9cursif de l\u2019arborescence pour g\u00e9rer fichiers et dossiers. Algorithme de Diff :</p> <p>Utilise la biblioth\u00e8que diff pour comparer deux versions d\u2019un fichier. G\u00e9n\u00e8re un \u201cunified diff\u201d pour refl\u00e9ter les changements. Si le diff est plus grand que le fichier lui-m\u00eame, on inclut directement le contenu complet. Recherche dans l\u2019Historique des Chats :</p> <p>useSearchFilter pour filtrer les conversations en fonction de mots-cl\u00e9s. Recherche insensible \u00e0 la casse sur les champs sp\u00e9cifi\u00e9s (ex.: description). S\u00e9lection de Template :</p> <p>Un prompt d\u00e9di\u00e9 starterTemplateSelectionPrompt demande au LLM de choisir un template en fonction des besoins du projet. Parse la r\u00e9ponse pour extraire le nom du template et le titre du projet. Clonage de Projet Git :</p> <p>Utilise isomorphic-git pour cloner le d\u00e9p\u00f4t dans WebContainer. Ignore les fichiers d\u00e9finis dans IGNORE_PATTERNS. Extrait le contenu et d\u00e9tecte les commandes potentielles pour l\u2019assistant. Gestion des Onglets :</p> <p>Liste d\u2019onglets ouverts dans EditorPanel. Permet d\u2019ajouter, supprimer et basculer entre les onglets. Affiche le fichier actif dans l\u2019\u00e9diteur. Flux de Donn\u00e9es : Interaction Utilisateur : Chat, \u00e9diteur, terminal, etc. Stores (Nano-stores) : chatStore, workbenchStore, themeStore, settingsStore, logStore. Composants React : Mise \u00e0 jour des composants abonn\u00e9es aux stores. API LLM : Envoi des requ\u00eates \u00e0 /api/chat pour obtenir des r\u00e9ponses selon le fournisseur choisi. WebContainer : Ex\u00e9cution de commandes shell, gestion de fichiers c\u00f4t\u00e9 client. IndexedDB : Persistance de l\u2019historique et des configurations. GitHub API : Clonage et publication du code sur GitHub. D\u00e9cisions de Conception Sp\u00e9cifiques : WebContainers : \u00c9vite le recours \u00e0 un serveur distant pour ex\u00e9cuter du code Node.js. API Streams : Diffuse le contenu g\u00e9n\u00e9r\u00e9 par le LLM (am\u00e9liore l\u2019exp\u00e9rience utilisateur). Structure modulaire : Favorise la maintenance et l\u2019extensibilit\u00e9. Framer Motion et Radix UI : Animations fluides et composants accessibles. IndexedDB : Permet de conserver l\u2019historique localement (mode hors connexion possible). UnoCSS : Approche \u201cutility-first\u201d pour des performances \u00e9lev\u00e9es. Gestion des Erreurs : try...catch : Enveloppe les appels aux APIs et aux stores. react-toastify : Affiche des messages d\u2019erreur contextuels \u00e0 l\u2019utilisateur. unreachable : G\u00e8re les cas qui ne devraient jamais se produire (assertions). S\u00e9curit\u00e9 : Cl\u00e9s API : Stock\u00e9es dans un cookie JSON, configurables via l\u2019UI ou des variables d\u2019environnement. Validation des URLs Git : Avant clonage pour \u00e9viter les sources non autoris\u00e9es. Nettoyage du Contenu : \u00c9vite les injections HTML/CSS. \u00c9chappement des Entr\u00e9es Utilisateur : Emp\u00eache les failles XSS. Performances : memo et useCallback : \u00c9vitent les rendus et cr\u00e9ations de fonctions inutiles. debounce : Limite la fr\u00e9quence d\u2019appels de certaines fonctions co\u00fbteuses. requestAnimationFrame : Optimise les animations. Promise.all : Ex\u00e9cute plusieurs t\u00e2ches en parall\u00e8le. Pagination : G\u00e8re les longs historiques de conversations. Nano-stores : L\u00e9gers et r\u00e9actifs pour la gestion de l\u2019\u00e9tat global. Accessibilit\u00e9 : Radix UI : Composants pr\u00eats pour l\u2019accessibilit\u00e9. Attributs ARIA : Meilleure navigation clavier et lecture d\u2019\u00e9cran. Contrastes Couleurs : Respecte les normes WCAG. Internationalisation : date-fns : G\u00e8re la locale pour le formatage des dates et heures. Possibilit\u00e9 de traductions : La structure peut prendre en charge des fichiers de ressources (non impl\u00e9ment\u00e9 \u00e0 ce stade). Tests : Vitest : Tests unitaires. @testing-library/react : Tests de composants et hooks. nanostores : Teste le comportement des stores. Documentation : MKDocs + Material for MkDocs : G\u00e9n\u00e9ration de la documentation \u00e0 partir de fichiers Markdown. Documentation technique : Pr\u00e9sente dans ce document, d\u00e9crivant l\u2019architecture, la logique et les choix de conception. Conclusion : NeuroCode-ia est une application web modulaire et extensible qui combine de nombreuses technologies (Remix, Vite, WebContainers, CodeMirror 6, etc.) pour offrir une exp\u00e9rience de d\u00e9veloppement assist\u00e9e par IA, directement dans le navigateur. La conception privil\u00e9gie la maintenabilit\u00e9, l\u2019\u00e9volutivit\u00e9 et la simplicit\u00e9 d\u2019usage, tout en assurant la persistance des donn\u00e9es (IndexedDB), la s\u00e9curit\u00e9 (cl\u00e9s API, validation d\u2019URL Git) et des performances optimales (memo, useCallback, etc.).</p> <p>Cette documentation technique et de conception donne une vue d\u2019ensemble des sp\u00e9cifications, de l\u2019architecture, des algorithmes cl\u00e9s et des d\u00e9cisions de conception, permettant \u00e0 tout nouveau d\u00e9veloppeur de prendre en main le projet rapidement et efficacement.</p>"}]}